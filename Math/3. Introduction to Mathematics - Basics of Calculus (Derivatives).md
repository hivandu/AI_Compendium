# 3. Introduction to Mathematics - Basics of Calculus (Derivatives)

**This article is section 3 of “[Hivan’s AI Compendium](https://medium.com/@hivan/list/hivans-ai-compendium-7ceb8b93d1a8) – Math Edition”**

![茶桁的AI秘籍Math_03](https://cdn.jsdelivr.net/gh/hivandu/notes/img/202408021620150.png)

Hi, everyone. I'm Hivan.

We need a few classes to review some basic mathematics concepts. Those who can recall them are very excellent, but if you can’t, it's okay—just follow my pace and you'll be fine.

In this class, we'll start by reviewing the basics of calculus (derivatives).

## Functions

First, let's review what a function is.

According to our memory or impression, a function is something where on the left side, we have a dependent variable `y` (often represented by `y` ), and on the right side, there's a function expression: $y = x + 1$, where the function expression contains an independent variable.

Let's look at four graphs:

![Graphs](https://files.mdnice.com/user/43981/fa27e701-ba74-4406-bcb2-7956f4188918.png)

These images include some that are functions and some that are not. Do you know which of these four images does not represent a function?

The one in the top left is not a function. Why? Because a single `x` value corresponds to two `y` values. For example, `x = 0` corresponds to two `y` values, `(1.0, -1.0)`. This does not meet the definition of a function. The other graphs are all function graphs. If you got this right, it means you haven't forgotten this part.

Functions come in many forms: linear, quadratic, trigonometric, exponential, logarithmic, multivariable functions, etc.

$$
\begin{align*}
& y = x^2 &
& y = \sin x \\
& y = e^x &
& y = \log_2 x \\
& z = x^2 + y^2 &
& ...
\end{align*}
$$

If we think abstractly, what can we understand a function as? It's a black box where you give it an input, and it provides an output according to its rules.

![Function as a Black Box](https://files.mdnice.com/user/43981/9f20645c-ef49-4ffd-9178-d01b67c74249.png)

Expanding a bit, do you know how the term "function" came to be in Chinese? Li Shanlan, a mathematician from the Qing Dynasty, saw the word "function" and felt that it couldn't be transliterated, unlike how Japanese transliterated every word. He wanted a Chinese name. A function is like a black box; you don't need to know its internal structure. You give it an input, and it gives you an output according to its rules. So, the character "函" represents a box or a container. For example, "石函" (stone box) and "剑函" (sword box) essentially mean "box."

Besides, Li Shanlan's work on the power series expansion of the square root, and the study of various trigonometric functions, inverse trigonometric functions, and logarithmic functions, became one of the greatest achievements in 19th-century Chinese mathematics.

Li Shanlan translated "function" as "函数" to indicate that this rule is enclosed in a black box. More abstractly, the number of independent variables can be many. The function's input doesn't have to be just one; it can be `n` of them. You can have 10 million or 1 million inputs. But the output is always one.

$$
x, y, \ldots \text{(independent variables)} \Rightarrow f(\text{function}) \Rightarrow z \text{(dependent variable)}
$$

This is called a multivariable function. If there is only one independent variable, it is called a univariable function.

## Derivatives

After a brief review of functions, let's talk about derivatives.

How can we understand derivatives? Let’s use a kinematics example to illustrate.

![Motion Graph](https://files.mdnice.com/user/43981/8b53b1a6-4a67-4402-b86a-b29dab8c535b.png)

First, let’s look at this graph. You can understand this coordinate system, right? In this Cartesian plane, the x-axis represents the independent variable, and the y-axis represents the dependent variable. In this motion problem, the x-axis (time) and y-axis (displacement) are represented as `(y: {displacement}, x: {time})`.

The motion graph is the black line. Consider points $P_0$ and $P$.

If we only look at this segment, the particle moves from $P_0$ to $P$, so it displaces $\Delta y$ in $\Delta x$ time. We can obtain the average velocity $\overline{v} = \frac{\Delta y}{\Delta x}$ .

As we keep moving $P$ closer to $P_0$, not by moving the particle but by bringing the points on the graph closer together, $\Delta y$ and $\Delta x$ decrease progressively until $P$ nearly coincides with $P_0$. When they coincide, the segment between $P$ and $P_0$ becomes a tangent line (red line) as $\Delta x$ approaches 0. This tangent line represents the instantaneous velocity at $P_0$. Based on the previous analysis, we get the formula for instantaneous velocity:

$$
\lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \tan a
$$

Here, $\lim$ denotes taking the limit, and $\Delta x \rightarrow 0$ means $\Delta x$ approaches 0. The formula indicates that when $\Delta x$ approaches 0, $\frac{\Delta y}{\Delta x}$ equals $\tan a$, which is the tangent value. That is, the ratio of the opposite side to the adjacent side in the angle $\angle a$, as shown in the diagram:

![Tangent](https://files.mdnice.com/user/43981/ac2df882-fb9f-4ddc-ae48-888ff4dec6a6.jpg)

This doesn't mean that the two line segments are $\Delta y$ and $\Delta x$, but rather that their ratio is equal, as they have degenerated into a tangent.

## Derivative and Differentiation (Univariable Functions)

Now we can introduce the formal definition of a derivative:

$$
y' = f'(x) = \lim_{h \to 0}\frac{f(x+h)-f(x)}{(x+h)-x}
$$

Don’t be intimidated by this mathematical formula. From my experience, many things that can be explained in plain language are given academic names and concepts that sound difficult. In reality, when you aren’t distracted by symbols and characters, these concepts are quite simple.

What does a derivative mean? Generally, derivatives are represented by `'`, such as in `y'` or `f'`. It denotes the derivative of a function. For example, `f'(x)` represents the derivative of `f(x)`. The definition is essentially our approximation process. We take two points, one at `x+h` and one at `x`, and as `h` approaches 0, the numerator is the difference between `f(x+h)` and `f(x)` (which is $\Delta y$), and the denominator is `(x+h) - x`, which is $\Delta x$. So, the derivative is found by determining the slope of the tangent at point $P_0$ on the graph.

Here are some related definitions:

- `y` is called the antiderivative of `y'`.
- `y'` is called the derivative function (or derivative) of `y`.
- Differentiation: When the independent variable `x` changes infinitesimally ($dx$), the change in the dependent variable `f(x)` ($df(x)$) is denoted by:

$$
df(x) = f'(x)dx
$$

The concept of differentiation is often confused with derivatives. Although they are related, they represent different meanings. For instance, in the above graph, the particle moves from $P_0$ to $P$ with an almost imperceptible change in $y$. The change in $x$ (the horizontal shift) results in a small change in $y$. When the independent variable $x$ increases or decreases slightly, $f(x)$ changes accordingly. The change in both the independent and dependent variables is denoted by `d+variable`. The `d` represents differentiation. So, $dx$ is how much $x$ changes (which is very small, approaching 0), and $df(x)$ is how much $f(x)$ changes correspondingly.

In neural networks, optimization and backpropagation involve propagating the derivative. Therefore, derivatives are a core concept.

Let’s look at some common derivatives for elementary functions:

$$
\begin{align*}
& C' = 0 & 
& (x^n)' = nx^{n-1} \\
& (\sin x)' = \cos x & 
& (\cos x)' = -\sin x \\
& (a^x)' =
 a^x \ln a &
& (\log_a x)' = \frac{1}{x \ln a} \\
& (e^x)' = e^x &
& (\ln x)' = \frac{1}{x} \\
\end{align*}
$$

Let's look at an example of differentiation. We will use the definition of a derivative to prove the derivative of one function: $(\sin x)' = \cos x$.

$$
\begin{align*}
Proof: & \quad (\sin x)' = \cos x \\
Proof: & \quad Let f(x) = \sin x \\
Because: & \quad f'(x) = (\sin x)' = \lim_{h \to 0} \frac{f(x+h) - f(x)}{(x+h) - x} \\
& \quad = \lim_{h \to 0} \frac{\sin(x+h) - \sin x}{h} \\
& \quad = \lim_{h \to 0} \frac{\sin x \cos h + \cos x \sin h - \sin x}{h} \\
& \quad = \lim_{h \to 0} \left(\frac{\sin x (\cos h - 1)}{h} + \frac{\cos x \sin h}{h}\right) \\
Because: & \quad \lim_{h \to 0} \frac{\cos h - 1}{h} = 0 \text{ and } \lim_{h \to 0} \frac{\sin h}{h} = 1 \\
Therefore: & \quad \text{Original expression} = \cos x
\end{align*}
$$

In the process of conversion, we used the trigonometric identity for the sum and difference of angles. If you're interested, you can [Google it](https://www.google.com/search?q=sum-to-product+identities&rlz=1C5CHFA_enCN920HK920&oq=sum-to-product+identities&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORiABDIHCAEQABiABDIICAIQABgWGB4yCAgDEAAYFhgeMggIBBAAGBYYHjIICAUQABgWGB4yCAgGEAAYFhgeMggIBxAAGBYYHjIICAgQABgWGB4yCAgJEAAYFhge0gEHNzA4ajBqMagCALACAA&sourceid=chrome&ie=UTF-8).

Let's explain this reasoning a bit. When `h` is infinitesimally small, $\cos h = 1$, and $\sin h = h$. With this step, the remaining parts become relatively easier to understand.

Next, let's discuss the rules for derivatives. If there are two functions, what is the derivative of their combination? There are essentially four cases: addition and subtraction, multiplication by a constant, multiplication of two functions, and division of two functions. The results are as follows:

$$
\begin{align*}
& \text{Derivative rules:} \\
& \text{If } u = u(x) \text{ and } v = v(x) \text{ are differentiable, then:} \\
& (1) \quad (u \pm v)' = u' \pm v' \\
& (2) \quad (cu)' = cu' \text{ (where } C \text{ is a constant)} \\
& (3) \quad (uv)' = u'v + uv' \\
& (4) \quad \left(\frac{u}{v}\right)' = \frac{u'v - uv'}{v^2} \text{ (where } v \ne 0)
\end{align*}
$$

I won't go through each case in detail here. It's best for everyone to review these rules and work through examples on your own after class; it shouldn't be too difficult.

Now, let's look at a proof related to the derivative of the product of two functions. We want to prove why the derivative of the product of $u$ and $v$ is given by $u'v + uv'$. 

$$
\begin{align*}
Proof: & \quad (uv)' = u'v + uv' \\
Proof: & \quad Let u = f(x) \text{ and } v = g(x) \\
& \quad (uv)' = (f(x)g(x))' = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x)g(x)}{h} \\
& \quad = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x+h)g(x) + f(x+h)g(x) - f(x)g(x)}{h} \\
& \quad = \lim_{h \to 0} \frac{f(x+h)(g(x+h) - g(x)) + g(x)(f(x+h) - f(x))}{h} \\
& \quad = \lim_{h \to 0} \left[f(x+h) \times \frac{g(x+h) - g(x)}{h} + g(x) \times \frac{f(x+h) - f(x)}{h}\right] \\
& \quad = f(x)g'(x) + g(x)f'(x) \\
& \quad \Rightarrow u'v + uv'
\end{align*}
$$

Similarly, we use the derivative definition for functions `f(x)` and `g(x)` to prove this. The derivative definition not only helps find the derivative of a specific function but also the derivative of the product of two functions.

In machine learning, derivatives are most commonly used in optimization problems involving the objective function or loss function. Whether in image processing or natural language processing, there is always an objective function to optimize. Optimization involves continuously differentiating and propagating the derivatives through the layers of a neural network, from the last layer to the previous ones. Derivatives are crucial in this process.

Have you heard of the term "Stochastic Gradient Descent"? Gradient descent is essentially differentiation. It continuously optimizes parameters in neural networks by subtracting the derivative value multiplied by the learning rate. Regardless of the optimization method, even if it's not stochastic gradient descent, it is fundamentally based on differentiation. Thus, wherever neural networks are used, differentiation is indispensable.

## Partial Derivatives

Derivatives are for single-variable functions, where there is only one independent variable. However, in nature, we often encounter multi-variable functions where one independent variable cannot produce output according to a predetermined rule; it requires multiple inputs.

For instance, $x_1, x_2, ..., x_n$. The function's mapping rule gives an output based on these inputs, which is called a multi-variable function. Multi-variable functions have a problem: if there are many independent variables, does the dependent variable have a derivative with respect to each independent variable? Indeed, each partial derivative of the dependent variable with respect to each independent variable is called a partial derivative.

Multi-variable function: $y = f(x_1, x_2, ..., x_n)$

- The dependent variable has a partial derivative with respect to each independent variable, called a partial derivative.

- Form: $\frac{\partial y}{\partial x_1},\frac{\partial y}{\partial x_2},...,\frac{\partial y}{\partial x_n}$

- How to compute: When solving for the partial derivative with respect to one variable, treat other variables as constants.

- Gradient: A vector composed of partial derivatives in various directions, $\frac{\partial y}{\partial x_1},\frac{\partial y}{\partial x_2},...,\frac{\partial y}{\partial x_n}$

- Gradient notation: $\nabla f(x_1, x_2, ..., x_n)$ or $grad f(x_1, x_2, ..., x_n)$

You might find multi-variable functions complex, especially with interactions among variables. However, they don’t affect each other; treat them as constants during computation.

The gradient is a vector composed of partial derivatives in various directions. A vector differs from a scalar. Scalars are just values without direction, such as having 2 apples or 3 pears. Vectors have both magnitude and direction, representing changes in various directions from a given point in space. For example, the vector (1,1,1) represents a change of 1 in each of the three coordinate directions in three-dimensional space.

In physics, velocity is a vector. Unlike the speed in daily life, which only has magnitude, velocity also has direction. In physics, velocity is described not only by its speed (e.g., 70 km/h) but also by its direction.

Let's look at an example:

$$
\begin{align*}
Example: & \text{Find the partial derivatives of } y=f(x_1, x_2) = x_1^2 + x_1x_2 + \sin x_2 \\
Solution: & \frac{\partial y}{\partial x_1} = 2x_1 + x_2 \\
& \frac{\partial y}{\partial x_2} = x_1 + \cos x_2 \\
\end{align*}
$$

We've previously discussed that when finding the partial derivative with respect to one variable, we treat other variables as constants.

What is the partial derivative of the function with respect to $x_1$? For $x_1^2$, according to the differentiation rules we've discussed, it is $2x_1$. That is: $(x^n)' = nx^{n-1}$. If you're not familiar with this, you might want to review the derivatives of basic elementary functions. These functions are frequently used, so it's good to be familiar with them.

Now, let's look at the second term, $x_1x_2$. Here, $x_2$ is treated as a constant, so it is equivalent to $Cx_1$, where $C$ is constant. The derivative of $Cx_1$ is $C$, which gives us $x_2$. For the last term, $\sin(x_2)$, since it does not contain $x_1$, it acts as a constant $C$ with respect to $x_1$, and the derivative of a constant is `0`. Why is the derivative of a constant `0`? Because if a function is constant, the value of $y$ does not change regardless of the value of $x$; it represents a horizontal line. 

For example, if `f(x) = 3`, no matter what `x` is, the function value is always `3`, which is a horizontal line. The rate of change of a horizontal line is `0`, and the derivative represents the rate of change.

Next, let's look at the partial derivative of `y` with respect to $x_2$. Using the same method, the first term does not contain $x_2$, so its derivative is `0`. The second term, $x_1 \cdot x_2$, treats $x_1$ as a constant, so the derivative is $x_1$. For the last term, $\sin(x_2)$, the derivative is $\cos(x_2)$, which we have previously proven.

This concludes our discussion on the basics of calculus in this introductory lesson on derivatives. However, the introduction is not over; we will cover linear algebra fundamentals (matrices), probability and statistics basics (random variables), and graph theory (concepts of graphs) in the next lesson.

Alright, everyone, see you in the next class!

Friends who need it, please click to subscribe: "[Hivan's AI Compendium](https://medium.com/@hivan/list/hivans-ai-compendium-7ceb8b93d1a8) "

https://medium.com/@hivan/list/hivans-ai-compendium-7ceb8b93d1a8